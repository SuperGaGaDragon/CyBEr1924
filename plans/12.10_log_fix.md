
## 12.10 关键问题修复方案

当前问题请查看12.10_log_check.md

### 问题 1：UI 不显示 worker 输出（阻塞）
**现象**：`/sessions/{id}` 返回的 snapshot 只有最开始 4 条 envelope（plan_created / user_command / coord_response / user_command），`SUBTASK_RESULT` 完全没有；前端一直卡在 `task1 loading`，timeline、worker output、reviewer column 都没有任何内容。
**日志**：`[Snapshot] Found 0 SUBTASK_RESULT envelopes` 出现在 `/command` 和 `/events` 的每一次 snapshot，即使之后 worker 连续写入 8 个 `SUBTASK_RESULT` 也没被读出；`build_session_snapshot` 似乎永远读不到子任务结果。
**根因推测**：
1. Snapshot 的读取总是在 worker 写入之前完成（/command 立即创建 snapshot），然后再读还是旧内容。
2. `envelopes.jsonl` 的写入没有强制 flush/fsync，写入后的内容在 OS 缓存里，reader 无法看到。
3. `_load_worker_outputs_since` 和 events 接口直接依赖 `orchestrator_state.worker_outputs` 缓存（而 snapshot 写入前可能没来得及更新），一旦状态文件丢失或缓存为空，就不会去重建（即使 envelopes.jsonl 有数据）。
**验证链路缺失**：
- `multi_agent_platform/sessions/.../logs/envelopes.jsonl` 的 tail 显示 `SUBTASK_RESULT` 被写入后才出现空 snapshot（见 12.10_log_check.md 的 `Sending SUBTASK_RESULT` + `Envelope log exists with X SUBTASK_RESULT entries`），说明数据确实落盘。
- 同一时间 `orchestrator_state.json` 的 `worker_outputs` 字段仍为空（日志中只出现“Cached output on subtask t1”但 snapshot 里 `worker_outputs` 仍为 `[]`），暗示状态文件写入和 snapshot 读取之间没有同步点。
- `build_session_snapshot` 读取 log 时打印的 `processing 4 envelopes` / `Found 0 SUBTASK_RESULT` 与 `tail` 下的实际 8 条 entries 相冲突，说明 snapshot 当前持有的 `state` 不是最新的 envelopes，因此需要明确定位 snapshot 调用链可以复现的步骤：1) `/sessions/{id}/command` → 2) `build_session_snapshot` reads `envelopes.jsonl` before worker writes results → 3) Worker logs show `SUBTASK_RESULT` entries appended → 4) `/sessions/{id}/events` still gets stale snapshot.
**复现建议**：
- 在本地/production 中重复：创建 session → 调用 `/sessions/{id}/command`（立刻返回 snapshot） → 轮询 `/events`；每次 snapshot 记录日志如 `[Snapshot] Processing X envelopes`，并同时 `tail -n 5` log file 来对比最新 envelope。这个对比可以说明 snapshot 何时读不到 SUBTASK_RESULT。
- 直接读取 `orchestrator_state.json` 中 `worker_outputs` 对比 `envelopes.jsonl`，同时记录写入时间戳，确认 `save_orchestrator_state()` 何时更新 worker outputs relative to snapshot request.

### 问题 2：Planner 不自动派遣章节任务 (novel mode)
**现象**：t1–t4 均完成，planner 在日志里 `DEBUG: Planner returned …`，且 `Decision: ACCEPT`，但是之后 coordinator 无任何动作，UI 甚至没有新的 requests。
**根因推测**：
1. Coordinator 在 novel mode 处理完前四个 subtask 之后，误认为所有 subtask 都 done 就退出了主循环，没触发 `planner.expand_chapters` 或把返回的章节 task 写回 `state.subtasks`。
2. 即便 planner 生成了 chapters，也可能只写到了 `artifacts` 里，没有被 snapshot/in-memory state 识别或重新派发给 worker。

### 展示运转过程.txt 的完成度能否解决当前问题？
- **结论：不能自动解决**。虽然该计划的 Phase 1.1 已增加 `message_bus.py` 里 envelope 写入的 `flush()`/`fsync()`、Phase 1.3~1.4 已强化 `_load_worker_outputs_since` 和 `_load_progress_events` 的容错，但当前生产日志仍旧显示 snapshot 读不到 `SUBTASK_RESULT`，说明：
  * 修复可能还没部署到正在运行的 Railway 实例；
  * 或者在同一进程/线程并发读写时仅靠 flush 还不够（snapshot 要么依赖过期的 `state`，要么读写冲突仍在发生）。
因此，尽快做下面的更深度补强仍然是必须的，同时需要明确部署校验流程（见下文）。

-------
### 解决建议
总要求：不要同时动后端和前端！
每次改动都要反复检查是否会引起更大的问题！
避免做大风险改动！

Phase 1. **保证 envelope 写入/读取的可见性**
   - 1.1 检查 `multi_agent_platform/message_bus.py` 是否已经强制 `f.flush(); os.fsync(f.fileno())`，并且所有 producer 用完文件后立即关闭。
   - 1.2 在 snapshot 构建前，重新打开 `envelopes.jsonl`，并在 `_read_log_entries` 里加上 `buffering=1`/`line buffering` 或者在读取前 `subprocess.run(["sync"])` 以确保最新内容；加入诊断日志 `tail -5`，便于确认 `SUBTASK_RESULT` 是否已经写入。
   - 1.3 `build_session_snapshot` 应在 `envelopes` 读取后打印 `len(envelopes)` 和最后一条 `payload_type`，以及 `state.worker_outputs` 的长度，以便确定是文件读不到还是 state 未更新。

   phase1已经完成

Phase 2. **调整 snapshot/events 的 orchestration**
   - 2.1 `/sessions/{id}/command` 在 spawn 后不要立即返回 snapshot。需要评估等待 200~500ms 是否会拖慢前端响应：这个延迟只在第一个 worker 输出尚未写入时才生效，达到两个目的（1) 给 worker 足够时间写入 `SUBTASK_RESULT`，(2) 避免前端立即收到空 snapshot 而开始高频轮询。必须确认现有 request timeout（FastAPI 默认 60s）和并发模型是否容忍这一等待，否则应改为 `asyncio.wait_for(save_orchestrator_state(...), timeout=0.4)` 但立即返回的 fallback snapshot。建议添加监控指标：`command_response_delay`，采集 `time.time()` difference between request start and snapshot return, 以确保绝大多数请求仍 < 1s。
   - 2.2 `api.py` 中 `_load_worker_outputs_since` / `_load_progress_events` 同步失败时要 fallback 到 `build_session_snapshot` 的解析结果（render `SUBTASK_RESULT` from `envelopes.jsonl`），避免完全依赖可能过期的 `state` 文件。
   - 2.3 禁止 `SessionSnapshotModel` 因 `agent` 枚举不符而拒绝数据（已在展示运转过程计划内提过，要确保部署的是宽松版本）。
   - 2.4 *已实现*：`/sessions/{id}/command` 在返回 snapshot 前会在执行完 `orch.execute_command` 后依次调用 `_refresh_snapshot_if_needed`（最多等待 400ms、重新读取 snapshot 直到 worker outputs 出现）并记录 `command_response_delay`；如果状态/日志缓存空缺，事件和 worker output API 都会 fallback 到 `build_session_snapshot` 的数据，确保前端至少能看到仓库里的 SUBTASK_RESULT。

phase 2 已完成

Phase 3. **恢复 Planner 章节派遣**
   - 3.1 ✅ **已完成**：在 `_append_chapter_tasks_from_planner_stub` 方法中添加了 plan 和 state 的持久化逻辑
      - 在 t4 完成后，planner 会被调用生成章节任务
      - 章节任务被添加到 `plan.subtasks` 后，**立即保存** plan 和 state
      - 这确保了主循环 `run_all_pending` 在下一次迭代时能看到新的章节任务
   - 3.2 ✅ **已完成**：添加了完整的诊断日志
      - `[Planner] Starting chapter expansion after t4 completion` - 开始章节扩展
      - `[Planner] Calling planner to generate chapter outline` - 调用 planner
      - `[Planner] Parsed X chapter candidates from outline` - 解析章节数量
      - `[Planner] After sanitization: X valid chapters` - 净化后的有效章节
      - `[Planner] Persisting plan with X new chapter tasks` - 持久化 plan
      - `[Planner] ✓ Plan and state persisted successfully` - 持久化成功

   **修改详情**：
   - 文件：`multi_agent_platform/run_flow.py`
   - 方法：`_append_chapter_tasks_from_planner_stub` (行 1219-1395)
   - 关键改动：
     1. 在章节任务添加后（第 1379-1383 行），调用 `self.save_state(session_id, plan)` 和 `self.save_orchestrator_state(state)` 保存更新后的 plan
     2. 添加了异常处理，确保保存失败不会崩溃
     3. 添加了全面的日志输出，便于追踪章节派遣过程

phase 3 已完成

Phase 4. **监控与验证**
   - 4.1 本地/CI 复用 `test_real_planner_mode.py` 的 `TestNovelModeFlow::test_snapshot_includes_worker_outputs`（若不存在，可新增子测试），mock `envelopes.jsonl` 里没有 `SUBTASK_RESULT` 到 snapshot 在 worker 写入之后再继续读取的场景。
   - 4.2 使用 `pytest test_unified_flow.py -k novel` 并观察 `SESSION_LOG_PATH` 的 `envelopes.jsonl`：测试断言 `SUBTASK_RESULT` entries 被 snapshot 读取，而且 novel mode 完成后 `state.subtasks` 包含 `chapter` 类型。
   - 4.3 如果 CI 已有失败记录（grep `Found 0 SUBTASK_RESULT` in `ci.log`/`test.log`），引用这些作为复现；若未有，创建一个 smoke test（`python -m scripts/create_session_and_poll --mode novel`），确保日志中既有 `plan_created` 也有 `SUBTASK_RESULT` 和自动派发章节。
   - Railway 上的 session：在调用 `/sessions/{id}/command` 之后查看 logs，确认 `command_response_delay` 新指标 < 1s，且 snapshot 日志中 `Found X SUBTASK_RESULT`（X 应 ≥ 4）和 `Worker_outputs count` 正确。

### 部署/版本校验
- 为验证生产环境是否部署了上述修复，需要比对当前仓库 `git describe --always` 或 `git rev-parse HEAD` 与 Railway release（`railway status`/`railway service list` 输出中的 Git SHA）是否一致；若不一致，说明部署依然是旧版。
- 若无法直接读取 Railway 的 Git SHA，可在本地 `git log -1`、`git show HEAD~1` 等对照与 `railway` CLI 提供的 `commit` 字段；同时在 Railway logs 中查找 `build` 阶段是否包含我们预计的 log 语句（`message_bus` flush, `planner chapter candidates`）。

### 后续动作
- 立刻在当前代码库里确认 `message_bus.py`、`api.py`、`run_flow.py` 的相关逻辑，追加诊断日志。
- 若发现生产环境代码与本地不一致，请同步并重新部署以覆盖旧版本。
- 完成上述后，重新打开展现 timeline、worker output、planner 章节 dispatch 的端到端 smoke 线程，必要时截图/记录日志以验证。
