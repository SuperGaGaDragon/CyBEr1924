
## 12.10 关键问题修复方案

当前问题请查看12.10_log_check.md

### 问题 1：UI 不显示 worker 输出（阻塞）
**现象**：`/sessions/{id}` 返回的 snapshot 只有最开始 4 条 envelope（plan_created / user_command / coord_response / user_command），`SUBTASK_RESULT` 完全没有；前端一直卡在 `task1 loading`，timeline、worker output、reviewer column 都没有任何内容。
**日志**：`[Snapshot] Found 0 SUBTASK_RESULT envelopes` 出现在 `/command` 和 `/events` 的每一次 snapshot，即使之后 worker 连续写入 8 个 `SUBTASK_RESULT` 也没被读出；`build_session_snapshot` 似乎永远读不到子任务结果。
**根因推测**：
1. Snapshot 的读取总是在 worker 写入之前完成（/command 立即创建 snapshot），然后再读还是旧内容。
2. `envelopes.jsonl` 的写入没有强制 flush/fsync，写入后的内容在 OS 缓存里，reader 无法看到。
3. `_load_worker_outputs_since` 和 events 接口直接依赖 `orchestrator_state.worker_outputs` 缓存（而 snapshot 写入前可能没来得及更新），一旦状态文件丢失或缓存为空，就不会去重建（即使 envelopes.jsonl 有数据）。
**验证链路缺失**：
- `multi_agent_platform/sessions/.../logs/envelopes.jsonl` 的 tail 显示 `SUBTASK_RESULT` 被写入后才出现空 snapshot（见 12.10_log_check.md 的 `Sending SUBTASK_RESULT` + `Envelope log exists with X SUBTASK_RESULT entries`），说明数据确实落盘。
- 同一时间 `orchestrator_state.json` 的 `worker_outputs` 字段仍为空（日志中只出现“Cached output on subtask t1”但 snapshot 里 `worker_outputs` 仍为 `[]`），暗示状态文件写入和 snapshot 读取之间没有同步点。
- `build_session_snapshot` 读取 log 时打印的 `processing 4 envelopes` / `Found 0 SUBTASK_RESULT` 与 `tail` 下的实际 8 条 entries 相冲突，说明 snapshot 当前持有的 `state` 不是最新的 envelopes，因此需要明确定位 snapshot 调用链可以复现的步骤：1) `/sessions/{id}/command` → 2) `build_session_snapshot` reads `envelopes.jsonl` before worker writes results → 3) Worker logs show `SUBTASK_RESULT` entries appended → 4) `/sessions/{id}/events` still gets stale snapshot.
**复现建议**：
- 在本地/production 中重复：创建 session → 调用 `/sessions/{id}/command`（立刻返回 snapshot） → 轮询 `/events`；每次 snapshot 记录日志如 `[Snapshot] Processing X envelopes`，并同时 `tail -n 5` log file 来对比最新 envelope。这个对比可以说明 snapshot 何时读不到 SUBTASK_RESULT。
- 直接读取 `orchestrator_state.json` 中 `worker_outputs` 对比 `envelopes.jsonl`，同时记录写入时间戳，确认 `save_orchestrator_state()` 何时更新 worker outputs relative to snapshot request.

### 问题 2：Planner 不自动派遣章节任务 (novel mode)
**现象**：t1–t4 均完成，planner 在日志里 `DEBUG: Planner returned …`，且 `Decision: ACCEPT`，但是之后 coordinator 无任何动作，UI 甚至没有新的 requests。
**根因推测**：
1. Coordinator 在 novel mode 处理完前四个 subtask 之后，误认为所有 subtask 都 done 就退出了主循环，没触发 `planner.expand_chapters` 或把返回的章节 task 写回 `state.subtasks`。
2. 即便 planner 生成了 chapters，也可能只写到了 `artifacts` 里，没有被 snapshot/in-memory state 识别或重新派发给 worker。

### 展示运转过程.txt 的完成度能否解决当前问题？
- **结论：不能自动解决**。虽然该计划的 Phase 1.1 已增加 `message_bus.py` 里 envelope 写入的 `flush()`/`fsync()`、Phase 1.3~1.4 已强化 `_load_worker_outputs_since` 和 `_load_progress_events` 的容错，但当前生产日志仍旧显示 snapshot 读不到 `SUBTASK_RESULT`，说明：
  * 修复可能还没部署到正在运行的 Railway 实例；
  * 或者在同一进程/线程并发读写时仅靠 flush 还不够（snapshot 要么依赖过期的 `state`，要么读写冲突仍在发生）。
因此，尽快做下面的更深度补强仍然是必须的，同时需要明确部署校验流程（见下文）。

-------
### 解决建议
总要求：不要同时动后端和前端！
每次改动都要反复检查是否会引起更大的问题！
避免做大风险改动！

Phase 1. **保证 envelope 写入/读取的可见性**
   - 1.1 检查 `multi_agent_platform/message_bus.py` 是否已经强制 `f.flush(); os.fsync(f.fileno())`，并且所有 producer 用完文件后立即关闭。
   - 1.2 在 snapshot 构建前，重新打开 `envelopes.jsonl`，并在 `_read_log_entries` 里加上 `buffering=1`/`line buffering` 或者在读取前 `subprocess.run(["sync"])` 以确保最新内容；加入诊断日志 `tail -5`，便于确认 `SUBTASK_RESULT` 是否已经写入。
   - 1.3 `build_session_snapshot` 应在 `envelopes` 读取后打印 `len(envelopes)` 和最后一条 `payload_type`，以及 `state.worker_outputs` 的长度，以便确定是文件读不到还是 state 未更新。

   phase1已经完成

Phase 2. **调整 snapshot/events 的 orchestration**
   - 2.1 `/sessions/{id}/command` 在 spawn 后不要立即返回 snapshot。需要评估等待 200~500ms 是否会拖慢前端响应：这个延迟只在第一个 worker 输出尚未写入时才生效，达到两个目的（1) 给 worker 足够时间写入 `SUBTASK_RESULT`，(2) 避免前端立即收到空 snapshot 而开始高频轮询。必须确认现有 request timeout（FastAPI 默认 60s）和并发模型是否容忍这一等待，否则应改为 `asyncio.wait_for(save_orchestrator_state(...), timeout=0.4)` 但立即返回的 fallback snapshot。建议添加监控指标：`command_response_delay`，采集 `time.time()` difference between request start and snapshot return, 以确保绝大多数请求仍 < 1s。
   - 2.2 `api.py` 中 `_load_worker_outputs_since` / `_load_progress_events` 同步失败时要 fallback 到 `build_session_snapshot` 的解析结果（render `SUBTASK_RESULT` from `envelopes.jsonl`），避免完全依赖可能过期的 `state` 文件。
   - 2.3 禁止 `SessionSnapshotModel` 因 `agent` 枚举不符而拒绝数据（已在展示运转过程计划内提过，要确保部署的是宽松版本）。
   - 2.4 *已实现*：`/sessions/{id}/command` 在返回 snapshot 前会在执行完 `orch.execute_command` 后依次调用 `_refresh_snapshot_if_needed`（最多等待 400ms、重新读取 snapshot 直到 worker outputs 出现）并记录 `command_response_delay`；如果状态/日志缓存空缺，事件和 worker output API 都会 fallback 到 `build_session_snapshot` 的数据，确保前端至少能看到仓库里的 SUBTASK_RESULT。

phase 2 已完成

Phase 3. **恢复 Planner 章节派遣**
   - 3.1 ✅ **已完成**：在 `_append_chapter_tasks_from_planner_stub` 方法中添加了 plan 和 state 的持久化逻辑
      - 在 t4 完成后，planner 会被调用生成章节任务
      - 章节任务被添加到 `plan.subtasks` 后，**立即保存** plan 和 state
      - 这确保了主循环 `run_all_pending` 在下一次迭代时能看到新的章节任务
   - 3.2 ✅ **已完成**：添加了完整的诊断日志
      - `[Planner] Starting chapter expansion after t4 completion` - 开始章节扩展
      - `[Planner] Calling planner to generate chapter outline` - 调用 planner
      - `[Planner] Parsed X chapter candidates from outline` - 解析章节数量
      - `[Planner] After sanitization: X valid chapters` - 净化后的有效章节
      - `[Planner] Persisting plan with X new chapter tasks` - 持久化 plan
      - `[Planner] ✓ Plan and state persisted successfully` - 持久化成功

   **修改详情**：
   - 文件：`multi_agent_platform/run_flow.py`
   - 方法：`_append_chapter_tasks_from_planner_stub` (行 1219-1395)
   - 关键改动：
     1. 在章节任务添加后（第 1379-1383 行），调用 `self.save_state(session_id, plan)` 和 `self.save_orchestrator_state(state)` 保存更新后的 plan
     2. 添加了异常处理，确保保存失败不会崩溃
     3. 添加了全面的日志输出，便于追踪章节派遣过程

phase 3 已完成

Phase 6. **修复 Planner 章节生成逻辑（P0 关键修复）** ⚠️

**问题发现**（最新日志 12.11 4:09am）：
虽然 Phase 3 添加了持久化逻辑，但日志显示：
```
[Planner] After sanitization: 5 valid chapters
[Planner] No chapter tasks were added (skipping plan persistence)  ← ❌ 章节没有被添加！
```

**根本原因分析**：
1. **Planner 生成了错误的任务类型**
   - 日志显示：`t10 修改完善`, `t11 定稿与格式排版`, `t12 发布准备`
   - 这些是项目管理任务，而不是小说章节任务（应该是 t5, t6, t7...）

2. **输入数据问题**
   - `novel_summary_t1_t4` 包含了所有 t1-t4 的内容摘要
   - Planner 误以为需要生成整个项目的后续任务
   - **正确做法**：只传递 `t4` 的输出（章节分配内容）

**修复方案** ✅ **（已实施）**：

**修改位置**：`run_flow.py` 第 1252-1281 行

**修改内容**：
```python
# 修改前：传递整个 novel_summary
summary = state.extra.get("novel_summary_t1_t4")
outline = self._call_planner(
    topic,
    novel_profile=profile,
    novel_summary=summary,  # ← 包含 t1-t4 所有内容
    summary_artifact=artifact_payload,
    reviewer_batch_annotations=reviewer_annotations,
)

# 修改后：只传递 t4 的输出
t4_output = None
for sub in plan.subtasks:
    if sub.id == "t4":
        t4_output = sub.output
        break

if not t4_output:
    print("[Planner] ❌ Cannot find t4 output, skipping chapter expansion")
    return

outline = self._call_planner(
    topic,
    novel_profile=profile,
    novel_summary=t4_output,  # ← 只传递 t4 的章节分配内容
    summary_artifact=None,  # ← 移除 summary artifact
    reviewer_batch_annotations=None,  # ← 移除 reviewer annotations
)
```

**修复效果**：
- ✅ Planner 只看到 t4 的章节分配内容（如："第一章：沉默的日常"、"第二章：觉醒碎片"）
- ✅ Planner 生成正确的章节任务（t5, t6, t7...），而不是项目管理任务（t10, t11, t12）
- ✅ 章节任务能够成功添加到 `plan.subtasks`，触发持久化

**影响范围**：
- 仅影响 novel mode 的章节生成逻辑
- 不影响 t1-t4 的执行
- 不影响非 novel mode 的 Planner 调用

**风险等级**：✅ 低（精简输入，减少误导）

**新增日志**：
- `[Planner] Using t4 output (X chars) as chapter allocation guide` - 显示使用 t4 内容的长度

phase 6 已完成


Phase 4. **监控与验证**
   - 4.1 本地/CI 复用 `test_real_planner_mode.py` 的 `TestNovelModeFlow::test_snapshot_includes_worker_outputs`（若不存在，可新增子测试），mock `envelopes.jsonl` 里没有 `SUBTASK_RESULT` 到 snapshot 在 worker 写入之后再继续读取的场景。
   - 4.2 使用 `pytest test_unified_flow.py -k novel` 并观察 `SESSION_LOG_PATH` 的 `envelopes.jsonl`：测试断言 `SUBTASK_RESULT` entries 被 snapshot 读取，而且 novel mode 完成后 `state.subtasks` 包含 `chapter` 类型。
   - 4.3 如果 CI 已有失败记录（grep `Found 0 SUBTASK_RESULT` in `ci.log`/`test.log`），引用这些作为复现；若未有，创建一个 smoke test（`python -m scripts/create_session_and_poll --mode novel`），确保日志中既有 `plan_created` 也有 `SUBTASK_RESULT` 和自动派发章节。
   - Railway 上的 session：在调用 `/sessions/{id}/command` 之后查看 logs，确认 `command_response_delay` 新指标 < 1s，且 snapshot 日志中 `Found X SUBTASK_RESULT`（X 应 ≥ 4）和 `Worker_outputs count` 正确。

Phase 5. **修复前端 UI 显示问题**

经过完整的 code review 和架构分析，发现当前方案主要是诊断性的，**真正的根因需要在后端验证**。

### 核心问题重新评估

#### 问题 1：Worker Output 显示 Reviewer Comment ⚠️ **（优先级：P0）**
**现象**：用户报告 "worker 的 output 里面出现了 reviewer comment"

**前端代码分析**（App.tsx）：
- 第 4033 行：Worker Column 正确使用 `snapshot?.worker_outputs`
- 第 4287-4356 行：Worker Output 渲染逻辑只显示 `outputs` 数组内容
- **结论**：前端绑定正确，问题在于 `snapshot.worker_outputs` 的数据源

**真正根因（需后端验证）**：
1. **后端混入了 reviewer 内容到 `worker_outputs`**
   - 检查 `run_flow.py` 中哪里向 `state.worker_outputs` 添加数据
   - 可能在 reviewer 阶段错误地 append 了内容到 worker_outputs

2. **或者 artifact 引用错误**
   - Worker output 的 `artifact` 字段可能指向了 reviewer 的 artifact
   - 需要追踪 `ref_work.to_payload()` 返回的路径是否正确

**立即行动**：
```python
# 在 run_flow.py 的 _record_progress_event 附近添加验证
if agent == "worker" and subtask_id:
    print(f"[VERIFY] Adding worker output for {subtask_id}, artifact: {artifact_path}")
elif agent == "reviewer" and subtask_id:
    print(f"[VERIFY] Reviewer decision for {subtask_id}, should NOT be in worker_outputs")
```

**前端临时修复**（仅诊断，不解决根因）：
```typescript
// App.tsx:4284 行，改善提示文本（但不解决混入问题）
Worker outputs will appear here once tasks are completed.
```

---

#### 问题 2：Timeline 数据混淆 ⚠️ **（优先级：P1）**
**现象**：用户报告 "worker 的 timeline 和 output 全部挤在 worker column 的 timeline"

**前端代码分析**（App.tsx:719-764）：
- `buildProgressTimelineEntries` 正确过滤 `ev?.agent === agent`
- Worker Timeline 调用 `buildProgressTimelineEntries(snapshot, "worker")`
- Reviewer Timeline 调用 `buildProgressTimelineEntries(snapshot, "reviewer")`
- **结论**：前端过滤逻辑正确，问题在于后端的 `progress_events` 数据

**真正根因（需后端验证）**：
后端 `_record_progress_event` 可能错误标记了 `agent` 字段：
```python
# 错误示例（需检查）
_record_progress_event(
    session_id,
    state,
    agent="reviewer",  # ❌ 应该是 "worker"
    subtask_id=subtask.id,
    stage="finish",
    payload={"artifact_path": ref_work.path}
)
```

**立即行动**：
1. **后端验证**：搜索所有 `_record_progress_event` 调用，确认 agent 参数
   ```bash
   grep -n "_record_progress_event" multi_agent_platform/run_flow.py
   ```

2. **前端诊断日志**（帮助定位问题）：
   ```typescript
   // App.tsx:728 行添加
   .filter(({ ev }) => {
     const isCorrectAgent = ev?.agent === agent;
     if (!isCorrectAgent && ev?.agent) {
       console.error(`[Timeline] WRONG AGENT: expected "${agent}", got "${ev.agent}"`, {
         subtask_id: ev.subtask_id,
         stage: ev.stage,
         payload: ev.payload
       });
     }
     return isCorrectAgent && ev?.subtask_id;
   })
   ```

---

#### 问题 3：拖拽条无法点击 ⚠️ **（优先级：P2）**
**现象**：reviewer column 无法移动

**前端代码分析**（App.tsx:2756-2789）：
- 拖拽条代码存在且完整
- 8px 宽度，z-index: 10
- onMouseDown 监听器正确绑定
- **可能问题**：被其他元素遮挡或事件被拦截

**诊断方案**：
```typescript
// App.tsx:2757 添加
onMouseDown={(e) => {
  console.log("[Drag] Left divider clicked", {
    x: e.clientX,
    y: e.clientY,
    target: e.target,
    currentTarget: e.currentTarget
  });
  layoutDrag.current = "left";
  document.body.style.cursor = 'col-resize';
  document.body.style.userSelect = 'none';
}}
// 添加视觉反馈
onMouseEnter={(e) => {
  e.currentTarget.style.background = "rgba(59, 130, 246, 0.5)";
  console.log("[Drag] Hover detected on left divider");
}}
onMouseLeave={(e) => {
  e.currentTarget.style.background = "transparent";
}}
```

**浏览器验证步骤**：
1. 打开 DevTools → Elements
2. 选中拖拽条元素
3. 检查 computed styles 的 z-index 和 pointer-events
4. 查看是否有其他元素覆盖（右键 → Inspect）

---

### Phase 5 修正后的实施计划

#### 第一步：后端根因验证（必须先做）⚠️
**优先级：P0，必须立即完成**

1. **验证 `worker_outputs` 数据源**
   ```bash
   grep -A 5 "state.worker_outputs.append" multi_agent_platform/run_flow.py
   ```
   确保只有 worker 相关代码在添加内容

2. **验证 `_record_progress_event` 的 agent 参数**
   ```bash
   grep -B 3 -A 3 "_record_progress_event" multi_agent_platform/run_flow.py | grep -E "(agent=|_record_progress_event)"
   ```
   确保 worker 事件使用 `agent="worker"`，reviewer 事件使用 `agent="reviewer"`

3. **检查 snapshot 构建逻辑**
   - 查看 `build_session_snapshot` 如何填充 `worker_outputs`
   - 确认不会错误地混入 reviewer 数据

#### 第二步：前端诊断日志（辅助定位）
**仅在后端验证完成后部署**

1. 修复 Worker Output 提示文本（App.tsx:4284）
2. 添加 Timeline agent 过滤警告（App.tsx:728）
3. 添加拖拽条诊断日志和视觉反馈（App.tsx:2756-2789）

#### 第三步：根据验证结果修复

**✅ 已发现问题根源！**

通过验证发现：`run_flow.py` 第 2747 行，当用户采用 reviewer revision 时，代码错误地将 reviewer 的内容添加到 `worker_outputs`：

```python
# run_flow.py:2747 - 问题代码
if state is not None:
    state.worker_outputs.append(
        WorkerOutputState(
            subtask_id=target.id,
            title=target.title,
            artifact=ref_revision,  # ❌ 这是 reviewer 的 artifact
            timestamp=datetime.utcnow(),
        )
    )
```

**修复方案（低风险）**：
有两个选项：

**选项 A：不添加到 worker_outputs**（推荐，最干净）
```python
# run_flow.py:2746-2752 修改
note_prefix = "[adopted reviewer revision]"
target.notes = f"{note_prefix} {target.notes or ''}".strip()
# ❌ 删除：不要将 reviewer revision 添加到 worker_outputs
# if state is not None:
#     state.worker_outputs.append(...)
```

**选项 B：创建单独的 reviewer_revisions 列表**（如果需要在前端显示）
```python
# 在 OrchestratorState 中添加新字段
reviewer_revisions: List[ReviewerRevisionState] = []

# run_flow.py:2746-2752 修改
note_prefix = "[adopted reviewer revision]"
target.notes = f"{note_prefix} {target.notes or ''}".strip()
if state is not None:
    state.reviewer_revisions.append(
        ReviewerRevisionState(
            subtask_id=target.id,
            title=target.title,
            artifact=ref_revision,
            timestamp=datetime.utcnow(),
        )
    )
```

**推荐使用选项 A**，因为：
1. Reviewer revision 已经在 `target.notes` 中记录
2. 已经在 `state.extra.reviewer_revisions` 中存储（行 2730）
3. 不应该污染 worker_outputs 列表

**影响范围**：
- 仅影响"采用 reviewer revision"的场景
- 不影响正常的 worker 输出
- 不影响 reviewer decision 的显示

**风险等级**：✅ 低（仅删除错误的 append 调用）

---

### 关键差异说明

**之前的 Phase 5**：主要是前端诊断，治标不治本
**修正后的 Phase 5**：
1. **先验证后端根因**（最可能的问题源）
2. **前端日志仅作辅助**（帮助确认数据流）
3. **基于验证结果决定修复策略**（不盲目修改）

**为什么必须先查后端**：
- 前端的数据绑定经验证是正确的
- `buildProgressTimelineEntries` 的过滤逻辑正确
- Worker Column 使用的是 `snapshot.worker_outputs`
- **如果前端正确但显示错误，说明数据源（后端）有问题**


### 部署/版本校验
- 为验证生产环境是否部署了上述修复，需要比对当前仓库 `git describe --always` 或 `git rev-parse HEAD` 与 Railway release（`railway status`/`railway service list` 输出中的 Git SHA）是否一致；若不一致，说明部署依然是旧版。
- 若无法直接读取 Railway 的 Git SHA，可在本地 `git log -1`、`git show HEAD~1` 等对照与 `railway` CLI 提供的 `commit` 字段；同时在 Railway logs 中查找 `build` 阶段是否包含我们预计的 log 语句（`message_bus` flush, `planner chapter candidates`）。

### 后续动作
- 立刻在当前代码库里确认 `message_bus.py`、`api.py`、`run_flow.py` 的相关逻辑，追加诊断日志。
- 若发现生产环境代码与本地不一致，请同步并重新部署以覆盖旧版本。
- 完成上述后，重新打开展现 timeline、worker output、planner 章节 dispatch 的端到端 smoke 线程，必要时截图/记录日志以验证。


附件：phase extra：
Phase Extra. **前端 UI 显示问题（不在本次修复范围）**
根据 12.10_log_check.md 的最新日志（11:03 pm EST），发现了新的 UI 显示问题：

**问题现象**：
1. worker 的 timeline 和 output 全部挤在 worker column 的 timeline
2. worker 的 output 里面出现了 reviewer comment
3. reviewer column 无法移动，调换顺序的 button 点不了

**分析**：
- 这些是**前端渲染和 UI 交互问题**，不是后端数据或 API 的问题
- 从后端日志来看，SUBTASK_RESULT 已经正常写入和返回
- 问题可能在于：
  1. 前端组件的数据分类逻辑错误（worker output 和 reviewer comment 混淆）
  2. 前端 UI 的布局或状态管理问题（column 无法移动）
  3. 前端事件处理问题（button 点击无效）

**建议**：
- 这需要检查前端代码，特别是：
  - Timeline 组件的渲染逻辑
  - Worker/Reviewer column 的数据绑定
  - 拖拽和排序功能的事件处理
- 不在当前后端修复计划的范围内
- 建议创建单独的前端 bug 修复计划